{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b59932ec-50e2-47a9-8108-abb422527010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e8a5aeb-86c0-42ea-bfb4-c1937451387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf26a75f-e1eb-45eb-b924-9ece37db5968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b2a2198-4cf3-4e2d-949e-61716a568ffe",
   "metadata": {},
   "source": [
    "# Temperature scaling baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72194783-6ca9-4430-99dc-75e5ac536484",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = [\n",
    "    {\n",
    "        'PATH': \"./data/climate_change/\",\n",
    "        'NAME': \"CCC\",\n",
    "        'COLUMN_NAME': 'FSL_BART',\n",
    "    }, {\n",
    "        'PATH': \"./data/topic_stance/\",\n",
    "        'NAME': \"TS_topic\",\n",
    "        'COLUMN_NAME': 'FSL_BART_topic',\n",
    "    }, {\n",
    "        'PATH': \"./data/topic_stance/\",\n",
    "        'NAME': \"TS_stance\",\n",
    "        'COLUMN_NAME': 'FSL_BART_stance',\n",
    "    }, {\n",
    "        'PATH': \"./data/depression/\",\n",
    "        'NAME': \"D\",\n",
    "        'COLUMN_NAME': 'FSL_BART',\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01a97333-f259-4ec4-b096-75d414065de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* 0 :\t CCC\n",
      "\t* 1 :\t TS_topic\n",
      "\t* 2 :\t TS_stance\n",
      "\t* 3 :\t D\n"
     ]
    }
   ],
   "source": [
    "for i, config in enumerate(config_list):\n",
    "    print('\\t*', i, ':\\t', config['NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af22f1c6-7b88-408d-ab57-1ebc75957797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D\n"
     ]
    }
   ],
   "source": [
    "config_index = 3\n",
    "config = config_list[config_index]\n",
    "print(config['NAME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfea613-5b61-49f6-913f-004b452a6e6e",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d1879a2-4e6b-491f-be68-5d1ae05252b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(os.path.join(config['PATH'], 'training.pkl'))\n",
    "test_df = pd.read_pickle(os.path.join(config['PATH'], 'testing.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cd508d9-1b1d-49ca-b415-aad1ebc403ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['NAME'] == \"TS_topic\": \n",
    "    with open(os.path.join(config['PATH'], 'claims_topic.json')) as file:\n",
    "        claims = json.load(file)\n",
    "        \n",
    "elif config['NAME'] == \"TS_stance\": \n",
    "    with open(os.path.join(config['PATH'], 'claims_stance.json')) as file:\n",
    "        claims = json.load(file)\n",
    "        \n",
    "else:\n",
    "    with open(os.path.join(config['PATH'], 'claims.json')) as file:\n",
    "        claims = json.load(file)\n",
    "\n",
    "class_descr = claims[\"class_descr\"]\n",
    "del claims[\"class_descr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d849c3-67b9-4272-ada3-993e1b8c0716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84dc9ac4-278b-4219-9597-8427226b0502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_scale(temperature, logits):\n",
    "    \"\"\"\n",
    "    Perform temperature scaling on logits\n",
    "    \"\"\"\n",
    "    return logits / temperature\n",
    "\n",
    "\n",
    "def set_temperature(logits, labels):\n",
    "    \"\"\"\n",
    "    Tune the tempearature of the model (using the validation set).\n",
    "    We're going to set it to optimize NLL.\n",
    "    valid_loader (DataLoader): validation set loader\n",
    "    \"\"\"\n",
    "    temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "    logits = torch.FloatTensor(logits)\n",
    "    labels = torch.FloatTensor(labels)\n",
    "    \n",
    "    loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Calculate NLL and ECE before temperature scaling\n",
    "    before_temperature_nll = loss_function(logits, labels).item()\n",
    "\n",
    "    # Next: optimize the temperature w.r.t. NLL\n",
    "    optimizer = optim.LBFGS([temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "    def eval():\n",
    "        optimizer.zero_grad()\n",
    "        scaled_logits = temperature_scale(temperature, logits)\n",
    "        try:\n",
    "            loss = loss_function(scaled_logits, labels)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "            print(logits)\n",
    "            print(temperature)\n",
    "            print(scaled_logits)\n",
    "            print(labels)\n",
    "            input()\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    optimizer.step(eval)\n",
    "\n",
    "    # Calculate NLL and ECE after temperature scaling\n",
    "    after_temperature_nll = loss_function(temperature_scale(temperature, logits), labels).item()\n",
    "    \n",
    "    return temperature\n",
    "\n",
    "def get_calibrated_predictions_TS(zsl_dict, models, claims):\n",
    "    new_dict = dict()\n",
    "    for t in claims:\n",
    "        new_dict[claims[t]] = temperature_scale(models[t], torch.FloatTensor([zsl_dict[claims[t]]]))[0].item()\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f82e977e-2be4-45a1-9d58-aab68ffda03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = [5, 10, 20, 40, 80, 160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8eb4ea9-df98-42f1-bb9e-b2aef6652fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/64 [00:00<?, ?it/s]/Users/sandrinechausson/Documents/easyclaimsdetection/venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 64/64 [00:02<00:00, 31.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 64/64 [00:01<00:00, 45.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 64/64 [00:01<00:00, 46.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 64/64 [00:01<00:00, 38.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 64/64 [00:01<00:00, 36.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 64/64 [00:02<00:00, 30.57it/s]\n"
     ]
    }
   ],
   "source": [
    "for samp_size in sample_sizes:\n",
    "    \n",
    "    print(samp_size)\n",
    "    \n",
    "    models = dict()\n",
    "    volumes = dict()\n",
    "    \n",
    "    total_dfs = list()\n",
    "    \n",
    "    for t in tqdm(claims):\n",
    "        if t == \"class_descr\": \n",
    "            continue\n",
    "            \n",
    "        if config[\"NAME\"] == \"CCC\":\n",
    "            class_idx = t[:3]\n",
    "        elif config[\"NAME\"] == \"TS_topic\":\n",
    "            class_idx = t[:1]\n",
    "        elif config[\"NAME\"] == \"TS_stance\":\n",
    "            class_idx = t[:2]\n",
    "        elif config[\"NAME\"] == \"D\":\n",
    "            class_idx = t.split(\"_\")[0]\n",
    "        \n",
    "        # Sample data\n",
    "        sub_pos = train_df[train_df[class_idx + \"_annot\"] == 1]\n",
    "        samp_pos = sub_pos.sample(min(samp_size, len(sub_pos)))\n",
    "        sub_neg = train_df[train_df[class_idx + \"_annot\"] == 0]\n",
    "        samp_neg = sub_neg.sample(min(samp_size, len(sub_neg)))\n",
    "        volumes[t] = (len(samp_pos), len(samp_neg))\n",
    "        total_dfs += [samp_pos, samp_neg]\n",
    "        \n",
    "        # Fit calibrator\n",
    "        X = [d[claims[t]] for d in samp_pos[config[\"COLUMN_NAME\"]].to_list()] + [d[claims[t]] for d in samp_neg[config[\"COLUMN_NAME\"]].to_list()]\n",
    "        y = samp_pos[class_idx + \"_annot\"].to_list() + samp_neg[class_idx + \"_annot\"].to_list()\n",
    "        calibrator = set_temperature(X, y)\n",
    "        models[t] = calibrator\n",
    "        \n",
    "    # Get predictions\n",
    "    if config[\"NAME\"] == \"TS_topic\":\n",
    "        test_df[\"Temp_Scaling_BART_\" + str(samp_size) + \"_topic\"] = test_df[config[\"COLUMN_NAME\"]].apply(lambda x: get_calibrated_predictions_TS(x, models, claims))\n",
    "    elif config[\"NAME\"] == \"TS_stance\":\n",
    "        test_df[\"Temp_Scaling_BART_\" + str(samp_size) + \"_stance\"] = test_df[config[\"COLUMN_NAME\"]].apply(lambda x: get_calibrated_predictions_TS(x, models, claims))\n",
    "    else:\n",
    "        test_df[\"Temp_Scaling_BART_\" + str(samp_size)] = test_df[config[\"COLUMN_NAME\"]].apply(lambda x: get_calibrated_predictions_TS(x, models, claims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fa24737-257e-4382-b6e3-b841cad96b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_pickle(config[\"PATH\"] + \"testing.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f37f8c9-c4a7-4c8e-8aa5-44792ff08c57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
